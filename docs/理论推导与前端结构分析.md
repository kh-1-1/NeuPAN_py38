# 理论推导与前端结构分析（与实现严格对齐，已修复乱码）

说明：为避免在部分终端/编辑器中出现乱码，本文统一使用 ASCII 可读的数学符号（如 ||G^T mu||_2、Proj_{...}、-> 等），并保留式号 (18)–(25) 与论文体排版风格。若导出到 LaTeX，请把 mu/lambda/tau/sigma/rho 等替换回希腊字母，式子结构与编号保持一致。

一) 惩罚化子问题与块坐标法（原式号保持）

目标是在后退地平线 H 内，对所有点—时刻 {(i,t)} 并行得到其潜在距离特征（LDF）变量 {mu_i^t, lambda_i^t}。每个点—时刻的子问题写为强凸的惩罚化形式：

(18)  Q1:  min_{mu_i^t, lambda_i^t}  C_DUNE(mu_i^t, lambda_i^t)
           s.t.  mu_i^t in K*_0,  ||lambda_i^t||_* <= 1

其中 K*_0 为对偶锥（多边形机器人时等价于非负正交锥），||.||_* 为对偶范数（二维取 l2 球）。惩罚函数：

(19)  C_DUNE(mu_i^t, lambda_i^t) = - (mu_i^t)^T (G e_{p,i}^t - h)
                                   + rho || (mu_i^t)^T G + (lambda_i^t)^T R_t(s_t) ||_2^2

采用不精确块坐标下降（PIBCD）进行迭代：第 j 次迭代先固定位于上一迭代的 mu^ 执行 lambda 更新，再固定位新的 lambda 执行 mu 更新。给定 mu_hat_i^t(j-1)，lambda 子问题为：

(20)  min_{lambda_i^t}  Xi_j(lambda_i^t)   s.t.  ||lambda_i^t||_* <= 1,
      其中  Xi_j(lambda_i^t) = || (mu_hat_i^t(j-1))^T G + (lambda_i^t)^T R_t(s_t) ||_2^2.

其单步投影梯度更新：

(21)  lambda_i^{t*} <- Proj_{||lambda||_*<=1}( lambda_i^t - eps * grad_{lambda_i^t} Xi_j(lambda_i^t) ).

随后固定 lambda_i^{t*}，mu 子问题为：

(22)  min_{mu_i^t}  Theta_j(mu_i^t)   s.t.  mu_i^t in K*_0,
      其中  Theta_j(mu_i^t) = rho || (mu_i^t)^T G + (lambda_i^{t*})^T R_t(s_t) ||_2^2
                              - (mu_i^t)^T (G e_{p,i}^t - h).

其单步投影梯度更新：

(23)  mu_i^{t*} <- Proj_{mu in K*_0}( mu_i^t - eps * grad_{mu_i^t} Theta_j(mu_i^t) ).

迭代映射视作序列：

(24)  mu_hat(0) -g1-> mu_hat(1) -g2-> ... -gJ-> mu_hat(J),

每个 g_j 由 (21)/(23) 的“梯度—投影”算子组成；在标准步长条件下，该序列收敛到 (18) 的最优解。

二) 与当前实现对齐的坐标化与层内算子（行布局）

为与实现严格一致，我们在前端内使用机器人坐标（行布局），并把旋转从算子里“吸收”出去：定义 y^t := R_t(s_t)^T lambda^t，则约束保持为 ||y^t||_2 <= 1，惩罚项中的 (lambda^t)^T R_t(s_t) 化为 (y^t)^T，使 (21)/(23) 在前端内具统一形态。设同一时刻 t 的点集合为 x in R^{N×2}，对偶变量堆叠为 mu in R^{N×E}、y in R^{N×2}，几何常量 G in R^{E×2}、h in R^{E×1} 为 buffer（不学习）。

初始化：两层 32 宽度的全连接 + ReLU 的轻量编码器得到 mu_0 = ReLU(FC(·->E))（非负初始化）与 y_0 = Tanh(FC(·->2))（l2 球内初始化）。

第 j 层包含两步：

- lambda-step（对应式(21) 的投影梯度上升）：
  y <- y + sigma * (mu @ G)；随后逐行投影到单位 l2 球  y <- y / max(1, ||y||_2)。

- mu-step（对应式(23) 的投影梯度下降）：
  a_row <- x @ G^T - h^T；  mu <- mu + tau * (a_row - (y @ G^T)).

两步均为“固定标量步长、无预条件”的线性—二次算子；不使用 a_row 自适应步长，也不做对角/行预条件。随后执行两级可行性恢复：
1) 在 mu-step 后加入有界的可学习近端残差 delta(z)（z = mu @ G，经轻量 MLP 输出），以缩放系数 alpha 做加性微调 mu <- mu + alpha * delta(z)。
2) 立即执行硬投影 Proj_{mu>=0, ||mu G||_2<=1}(·)：先做锥投影 mu <- max(mu, 0)，再以 v = mu @ G 逐行缩放到 ||v||_2 <= 1。层末再做一次幂等安全投影兜底，从而在层与层之间维持“对偶可行性不变量”。当 alpha=0 时，网络退化为严格的固定步长 PDHG 展开。

三) DUNE 层：时域拼接与旋转恢复

DUNE 在前端外部处理时域与旋转：把各时刻点流拼接为一批送入前端，前向后按时刻切分回 mu^t，并计算 lam^t = - R_t(s_t) @ G^T @ mu^t 以恢复全局量，供目标距离与下游正则器使用。目标距离（逐点）为 d = mu^T (G p - h)；其最小值作为碰撞裕度指标。DUNE 还可基于距离与方向采样策略选点以控制时延。

四) 训练目标与诊断正则（原式(25) + 我们的正则）

以给定几何 [G,h] 与采样点 e_p 生成的最优解 mu* 为监督，采用综合损失：

(25)  L(mu_hat, mu*) = ||mu_hat - mu*||_2^2 + ||f_o(mu_hat) - f_o(mu*)||_2^2
                        + ||f_a(mu_hat) - f_a(mu*)||_2^2 + ||f_b(mu_hat) - f_b(mu*)||_2^2,

其中 f_o(mu) = mu^T (G e_p - h)，f_a(mu) = - mu^T G R_t^T，f_b(mu) = mu^T G R_t^T e_p - mu^T h。

为在有限展开步数 J 下输出贴近可行且近最优的解，训练期在“硬投影之前”的 mu 上加入两类诊断正则并回传：

- 几何可行性正则 L_constr：等价于到可行集 C = { mu>=0, ||G^T mu||_2<=1 } 的平方距离。实现为对 max(||G^T mu||_2 - 1, 0)^2 与 (min(mu, 0))^2 取均值。
- 相对 KKT 残差 L_kkt：刻画一阶最优性。记 a = G x - h，Gy = G (G^T mu)，s = ReLU(-mu)，令 r = -a + rho_k * Gy - s，并以尺度不变的相对范数 L_kkt = mean( (||r||_2 / (||a||_2 + eps))^2 ) 度量，避免不同几何/尺度的主导效应。

五) 稳定性与步长条件（匹配“固定步长、无预条件”）

每层是“投影 o 固定步长原/对偶梯度映射 o 有界残差 o 投影”的复合；在满足标准 PDHG 条件 tau * sigma * ||G||_2^2 < 1 时，层算子为平均型/非扩张映射。残差后随即投影保证了当 alpha 适度时整体稳定且可解释。末端幂等投影提供分布外兜底；训练期 L_constr/L_kkt 软驱动输出靠近可行且近最优区域，使投影成为微调而非强修复。

附注：如需严格 LaTeX 版，请将 mu/lambda/tau/sigma/rho 等替换回希腊字母，并把 (18)–(25) 放入数学模式；公式结构与编号保持不变。

六) 并行性说明（避免歧义）

本工作中的“并行”主要体现在两处：(i) 子问题的批并行——所有点 i（以及在 DUNE 中按时刻拼接后的所有点）在同一批中一次性更新；(ii) 算子上的复合视角——每个展开步内由 g1（lambda-step）与 g2（mu-step）组成的复合映射 g = g2 ∘ g1 将 (lambda^k, mu^k) 共同推进到 (lambda^{k+1}, mu^{k+1})。实现层面我们采用 Gauss–Seidel 式的交替更新（先 y 再 mu，mu 使用 y 的新值），与原文式(21)/(23) 完全一致；这是标准 PDHG/PI-BCD 的稳健写法。若从算子图角度描述，可以称其“在一步内联合推进”，但数值上仍是先后执行以保证稳定性。

如需“同时更新（Jacobi）”的变体，可将 mu-step 中用到的 y @ G^T 换成上一迭代的 y^k：

  y^{k+1} = Proj_{||.||_2<=1}( y^k + sigma * (mu^k @ G) ),
  mu^{k+1} = Proj_{mu>=0, ||G^T mu||_2<=1}( mu^k + tau * (x @ G^T - h^T - (y^k @ G^T)) ).

该“Jacobi-PDHG”在步长上通常更保守（收敛半径略紧），好处是更新式对实现顺序不敏感；我们当前默认采用更稳的 Gauss–Seidel 式交替更新。
