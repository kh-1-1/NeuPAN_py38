# 基于近端学习交替网络的移动机器人自主导航研究

## 2. 论文的研究内容、研究目标,以及拟解决的关键科学问题

### 2.1 研究内容与目标

本课题面向充电机器人在非结构化复杂环境中柔性补电的自主导航问题。传统导航系统采用感知-建图-规划-控制的串联架构,各模块误差逐级累积,影响整体性能;现有端到端学习方法虽能直接从传感器映射到控制动作,但神经网络输出缺乏物理约束保证,可能导致优化问题不可行。针对这些问题,本研究提出基于近端学习交替网络PLAN的自主导航方法,该方法通过交替优化可解释的原始对偶近端学习网络PDPL-Net和可学习参数神经正则化运动规划器LNRMP,将优化理论与深度学习深度融合,在保证实时性的前提下实现高精度避障和鲁棒导航。其研究内容主要为：

1) 基于MPC的无图导航系统约束建模

针对充电机器人在非结构化动态环境中,传统方法依赖预先建图和全局规划。本研究拟采用模型预测控制框架,建立无需预先建图的端到端导航系统,仅依靠实时激光雷达原始点云数据完成自主导航。通过这种约束建模方式,将复杂的无图导航问题转化为结构化的凸优化问题,为后续神经网络感知和在线学习提供可解释的优化框架。

2) 基于神经优化展开的点云潜在距离场感知与对偶生成

针对传统黑盒神经网络生成的对偶变量可能违反物理约束,导致后续优化问题不可行或求解失败。本研究提出原始对偶近端学习网络(Primal-Dual Proximal Learning Network,PDPL-Net),将经典的原始对偶混合梯度算法展开为可学习的神经网络层。将优化算法的迭代步骤显式嵌入网络结构,每一步包含对偶更新、原始更新和可行性投影三个操作,使网络的计算过程具有明确的物理意义。通过引入零参数硬投影机制,严格保证输出的对偶变量满足非负性和范数约束,确保后续优化问题的可行性。同时在展开步骤中引入可学习的近端算子,通过轻量级神经网络学习残差修正,在保持约束可行性的前提下提升特征表达能力。这种设计兼顾了设计兼顾了优化理论的严谨性和神经网络的学习能力,实现从障碍物点云到潜在距离场对偶变量的可解释映射。

3) 基于模型驱动与强化学习增强的动态环境实时自主导航

针对动态环境中优化参数人工设定存在的场景依赖性强、调参成本高、泛化能力弱等问题,本研究拟采用基于在线学习的可学习参数神经正则化运动规划器LNRMP。该模块在MPC框架下求解包含运动学约束和避障约束的凸优化问题,生成最优控制序列,其性能高度依赖于状态跟踪权重、控制平滑权重、避障权重以及安全距离上下限等关键参数的配置。传统方法中这些参数由人工设定,不同场景需要不同配置,调参过程耗时且难以保证最优性。本研究将这些参数转化为可学习变量,通过端到端可微分的优化框架,利用PyTorch自动微分机制计算参数梯度。在实际导航过程中,根据机器人是否成功到达目标、是否发生碰撞、轨迹是否平滑、能量消耗等多维度反馈信号构建损失函数,通过梯度下降自动调整参数。这种自适应机制借鉴强化学习思想,在线学习无需离线训练数据,能够根据环境特征动态优化参数配置,在保障安全性的前提下加速求解过程,提升系统对动态环境的快速响应能力。

4) 搭建基于ROS的仿真实验平台

针对充电机器人在非结构化复杂环境中柔性补电的自主导航问题,本研究拟搭建完整的仿真实验平台和真实机器人测试系统。硬件层面,基于阿克曼转向运动学模型搭建研究平台,配置激光雷达传感器和车载计算机。软件层面,基于ROS机器人操作系统构建完整的导航系统框架。仿真实验采用IR-SIM平台,构建凸障碍物、走廊、动态障碍物、非凸障碍物、路径跟踪、倒车入库等多种典型场景,测试算法在不同复杂度环境下的性能表现。真实环境测试室外广场、停车场等场景进行,验证算法的实时性、安全性和鲁棒性。

### 2.2 拟解决的关键科学问题

1. 针对充电机器人在非结构化动态环境中，采用MPC框架，将复杂的无图点级非凸优化问题转化为结构化的凸优化问题，进行系统建模。
2. 针对传统黑盒神经网络生成的对偶变量可能违反物理约束,导致后续优化问题不可行或求解失败。本研究提出原始对偶近端学习网络(Primal-Dual Proximal Learning Network,PDPL-Net),将经典的原始对偶混合梯度算法展开为可学习的神经网络层，实现从障碍物点云到潜在距离场对偶变量的可解释映射。
3. 针对动态环境中优化参数人工设定存在的场景依赖性强、调参成本高、泛化能力弱等问题,本研究拟采用基于在线学习的可学习参数神经正则化运动规划器LNRMP，完成实时避障与自主导航。
---

## 3. 拟采取的研究方案及可行性分析

### 3.1 总体研究方案

本研究采用理论分析与实验验证相结合的研究方法,遵循"理论建模→算法设计→仿真验证→实物部署"的技术路线。首先在理论层面建立基于MPC的无图导航系统约束建模框架,明确阿克曼转向运动学模型的线性化形式,严格推导运动学约束、避障约束和控制输入约束的数学表达。然后设计PLAN交替优化框架,通过协调PDPL-Net和LNRMP两个模块实现端到端导航。在PLAN的协调下,PDPL-Net首先将障碍物点云映射为潜在距离场的对偶变量,通过原始对偶混合梯度算法展开和硬投影机制保证对偶可行性;LNRMP随后基于对偶变量求解包含运动学约束和避障约束的凸优化问题,生成最优控制序列,同时将优化参数转化为可学习变量,通过在线学习机制根据任务反馈自动调整。两个模块交替迭代优化,直至收敛或达到最大迭代次数,从而高效求解带有大量点级碰撞避免约束的优化问题,实现实时自主导航。最后在仿真和真实环境中进行全面验证,评估算法的实时性、安全性和鲁棒性。

### 3.2 技术方案

本研究技术方案如下,与2.1节研究内容严格对应:

#### 1) 基于MPC的无图导航系统约束建模

本研究将采用模型预测控制框架建立无图导航系统的约束优化问题,技术路线如图2所示:

```
┌─────────────────────────────────────────────────────────────────┐
│                   阿克曼转向机器人运动学建模                      │
├─────────────────────────────────────────────────────────────────┤
│  • 定义状态向量: s = (x, y, θ)                                   │
│  • 定义控制向量: u = (v, δ)                                      │
│  • 建立非线性运动学方程: s_{t+1} = f(s_t, u_t)                   │
└─────────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────────┐
│                      运动学模型线性化                            │
├─────────────────────────────────────────────────────────────────┤
│  • 在名义轨迹附近进行一阶泰勒展开                                │
│  • 得到线性时变系统: s_{t+1} = A_t·s_t + B_t·u_t + C_t          │
│  • 系数矩阵A_t, B_t依赖于名义轨迹                                │
└─────────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────────┐
│                    避障约束双凸重构                              │
├─────────────────────────────────────────────────────────────────┤
│  • 机器人建模为凸多边形,障碍物表示为点云                         │
│  • 定义潜在距离函数: d(s_t, p_i) ≥ d_min                        │
│  • 引入对偶变量μ(边的贡献权重)和辅助变量λ(方向向量)              │
│  • 转化为双凸约束,满足非负性约束和范数约束                       │
└─────────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────────┐
│                   控制输入约束定义                               │
├─────────────────────────────────────────────────────────────────┤
│  • 速度边界约束: v_min ≤ v_t ≤ v_max, δ_min ≤ δ_t ≤ δ_max      │
│  • 加速度边界约束: |u_{t+1} - u_t| ≤ Δu_max                     │
│  • 保证轨迹平滑性和可执行性                                      │
└─────────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────────┐
│              构建结构化凸优化问题                                │
├─────────────────────────────────────────────────────────────────┤
│  min  J(s, u) = Σ ||s_t - s_ref||² + ||u_t||² + C_obs(s,μ,λ)   │
│  s.t. 运动学约束 + 避障约束 + 控制约束                           │
│  输出: 可解释的MPC优化框架                                       │
└─────────────────────────────────────────────────────────────────┘
```

图2 MPC约束建模技术路线

本研究首先针对阿克曼转向机器人建立运动学模型,采用状态向量s=(x,y,θ)表示位姿、控制向量u=(v,δ)表示线速度和转向角,在名义轨迹附近进行一阶泰勒展开将非线性模型线性化为线性时变系统s_{t+1}=A_t·s_t+B_t·u_t+C_t;在避障约束建模方面,采用点级碰撞避免策略,将机器人建模为凸多边形、障碍物表示为点云,引入对偶变量μ表示机器人凸包每条边的贡献权重、辅助变量λ表示障碍物点的方向向量,将非凸距离约束重构为双凸约束(满足μ≥0和||λ||≤1),支持交替优化高效求解;在控制输入约束方面,定义速度边界约束限制v和δ在物理可行范围内、加速度边界约束限制相邻时刻控制输入变化率|u_{t+1}-u_t|≤Δu_max,保证轨迹平滑性和可执行性;最终将复杂的无图导航问题转化为包含线性动力学约束、双凸避障约束和凸控制约束的结构化凸优化问题,为后续神经网络感知和在线学习提供可解释的优化框架。

#### 2) 基于神经优化展开的点云潜在距离场感知与对偶生成

本研究将设计PDPL-Net网络,实现从障碍物点云到潜在距离场对偶变量的可解释映射,技术路线如图3所示:

```
┌─────────────────────────────────────────────────────────────────┐
│                      输入: 障碍物点云                            │
│                   P = {p_i} ∈ R^{N×2} (机器人坐标系)             │
└─────────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────────┐
│                    特征编码器 (Encoder)                          │
├─────────────────────────────────────────────────────────────────┤
│  • 两层全连接网络 + ReLU激活                                     │
│  • 点云坐标 → 高维特征向量                                       │
│  • 输出分为两路:                                                 │
│    - μ初始化路径: FC + ReLU (保证非负性)                         │
│    - λ初始化路径: FC + Tanh (约束在单位球)                       │
└─────────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────────┐
│              PDHG展开层 (J=4-6步迭代)                            │
├─────────────────────────────────────────────────────────────────┤
│  第j步迭代 (j=1,2,...,J):                                        │
│  ┌───────────────────────────────────────────────────────────┐  │
│  │ 1. 对偶更新: μ^{j} ← μ^{j-1} + τ·∇_μ Θ(μ,λ^{j-1})        │  │
│  │ 2. 原始更新: λ^{j} ← λ^{j-1} - σ·∇_λ Ξ(μ^{j},λ)          │  │
│  │ 3. 可学习近端算子: 残差修正 (单层FC,参数量<10%)            │  │
│  │ 4. 可行性投影: 投影到约束集合                              │  │
│  └───────────────────────────────────────────────────────────┘  │
└─────────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────────┐
│                  硬投影层 (Hard Projection)                      │
├─────────────────────────────────────────────────────────────────┤
│  • 对偶变量μ: ReLU投影 → μ ≥ 0 (非负性)                         │
│  • 辅助变量λ: L2归一化 → ||λ|| ≤ 1 (单位球)                     │
│  • 零参数,纯数学投影操作                                         │
└─────────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────────┐
│              输出: 对偶变量 μ, λ (满足物理约束)                  │
└─────────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────────┐
│                      模型训练策略                                │
├─────────────────────────────────────────────────────────────────┤
│  • 训练数据: 随机生成位姿+点云,凸优化求解器获得真值              │
│  • 损失函数: L = L_MSE(μ) + L_MSE(d) + λ_KKT·L_KKT              │
│  • 优化器: Adam, lr=0.001, batch=256, epochs=2500               │
│  • 评估指标: 对偶可行性违反率、距离估计精度、推理时延            │
└─────────────────────────────────────────────────────────────────┘
```

图3 PDPL-Net网络设计与训练技术路线

本研究设计PDPL-Net网络,首先通过特征编码器(两层全连接+ReLU)将点云坐标映射为高维特征向量,输出分为两路分别生成对偶变量μ的初始值(FC+ReLU保证非负性)和辅助变量λ的初始值(FC+Tanh约束在单位球);然后进入PDHG展开层执行J步迭代优化(J=4-6),每步包含对偶更新(固定λ梯度上升更新μ)、原始更新(固定μ梯度下降更新λ)、可学习近端算子(单层FC学习残差修正,参数量<10%)和可行性投影四个操作,通过J步展开模拟优化算法迭代过程;接着通过硬投影层保证输出严格满足物理约束,对μ采用ReLU投影保证μ≥0、对λ采用L2归一化保证||λ||≤1,该层零参数纯数学投影将对偶可行性违反率从10%降至1%以下;最后在训练阶段,通过随机生成位姿和点云、使用CLARABEL求解器获得真值对偶变量,采用三项损失函数L_MSE(μ)+L_MSE(d)+λ_KKT·L_KKT(KKT残差使映射更接近最优解),使用Adam优化器(lr=0.001,batch=256,epochs=2500)训练,评估指标包括对偶可行性违反率、距离估计精度和推理时延,确保网络在保证可行性前提下实现高精度和实时性。

#### 3) 基于模型驱动与强化学习增强的动态环境实时自主导航

本研究将设计LNRMP模块,在MPC框架下求解包含运动学约束和避障约束的凸优化问题,生成最优控制序列,技术路线如图4所示:

```
┌─────────────────────────────────────────────────────────────────┐
│                  定义MPC优化问题                                 │
├─────────────────────────────────────────────────────────────────┤
│  min J = q_s·||s-s_ref||² + p_u·||u||² + η·C_obs(s,μ,λ)        │
│  s.t. 运动学约束 + 避障约束 + 控制约束                           │
│  传统方法: q_s, p_u, η, d_max, d_min 人工设定                   │
└─────────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────────┐
│              可学习参数定义与初始化                              │
├─────────────────────────────────────────────────────────────────┤
│  • 状态跟踪权重: q_s ∈ [0.01, 5.0]                              │
│  • 控制平滑权重: p_u ∈ [0.1, 10.0]                              │
│  • 避障权重: η ∈ [1.0, 50.0]                                    │
│  • 安全距离: d_max ∈ [0.1, 2.0], d_min ∈ [0.01, 0.5]           │
│  • 初始化为默认值,定义取值范围约束                               │
└─────────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────────┐
│          基于cvxpylayers的可微分优化求解                         │
├─────────────────────────────────────────────────────────────────┤
│  • 输入: 机器人状态s, 参考轨迹s_ref, 对偶变量μ,λ                │
│  • 求解: 凸优化问题 (CLARABEL求解器)                            │
│  • 输出: 最优控制序列u*, 最优状态轨迹s*                          │
│  • 关键: 整个求解过程可微分,支持梯度反向传播                     │
└─────────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────────┐
│                执行控制并收集任务反馈                            │
├─────────────────────────────────────────────────────────────────┤
│  • 执行第一步控制u*[0],机器人移动到新状态                        │
│  • 收集反馈: 是否碰撞、是否到达、轨迹平滑度、能量消耗            │
└─────────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────────┐
│              计算多目标任务损失函数                              │
├─────────────────────────────────────────────────────────────────┤
│  L_total = w1·L_distance + w2·L_smoothness + w3·L_energy +      │
│            w4·L_time + w5·L_tracking                             │
│  • L_distance: 碰撞惩罚,距离过近惩罚                             │
│  • L_smoothness: 急转弯、急加速惩罚                              │
│  • L_energy: 高速行驶、频繁加速惩罚                              │
│  • L_time: 快速到达奖励                                          │
│  • L_tracking: 偏离参考轨迹惩罚                                  │
└─────────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────────┐
│          通过自动微分计算梯度并更新参数                          │
├─────────────────────────────────────────────────────────────────┤
│  • 计算梯度: ∇_θ L_total (θ = {q_s, p_u, η, d_max, d_min})      │
│  • 梯度下降: θ ← θ - α·∇_θ L_total                              │
│  • 学习率: α = 0.01, 指数衰减策略                                │
│  • 参数投影: 投影到预定义取值范围                                │
└─────────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────────┐
│                  环境自适应导航                                  │
├─────────────────────────────────────────────────────────────────┤
│  • 开阔环境: 降低η, 提高速度 → 快速导航                         │
│  • 狭窄环境: 提高η, 降低速度 → 安全导航                         │
│  • 动态障碍物: 调整d_max, d_min → 平衡安全与效率                │
└─────────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────────┐
│            ROI选择与方向性增强采样 (辅助策略)                    │
├─────────────────────────────────────────────────────────────────┤
│  第一级: ROI选择 (可达性圆锥)                                    │
│  • 最大可达距离: d_reach = v_max × T × dt                       │
│  • 动态视野角度: θ_fov (根据路径曲率调整)                        │
│  • 筛选可达性圆锥内的障碍物点                                    │
│                              ↓                                   │
│  第二级: 方向性采样                                              │
│  • 前方180度视野划分为若干角度区间                               │
│  • 每个区间选择最近的K个点 (K=2-3)                               │
│  • 点云规模: 数千 → 数十                                         │
└─────────────────────────────────────────────────────────────────┘
```

图4 LNRMP在线学习与自适应导航技术路线

本研究在MPC框架下定义优化问题,目标函数包括状态跟踪误差项q_s·||s-s_ref||²、控制输入平滑项p_u·||u||²和避障成本项η·C_obs,将传统方法中人工设定的权重参数q_s、p_u、η以及安全距离阈值d_max、d_min转化为可学习变量,初始化为默认值并定义取值范围约束(如q_s∈[0.01,5.0]、η∈[1.0,50.0]);基于cvxpylayers实现可微分优化求解,将凸优化问题嵌入PyTorch计算图,输入机器人状态s、参考轨迹s_ref和对偶变量μ、λ,使用CLARABEL求解器输出最优控制序列u*和状态轨迹s*,执行第一步控制u*[0]后收集任务反馈(碰撞、到达、平滑度、能量消耗);定义多目标任务损失函数L_total=w1·L_distance+w2·L_smoothness+w3·L_energy+w4·L_time+w5·L_tracking,通过PyTorch自动微分计算梯度∇_θ L_total(θ={q_s,p_u,η,d_max,d_min}),使用梯度下降更新参数θ←θ-α·∇_θ L_total(α=0.01,指数衰减),更新后投影到预定义范围保证物理合理性;通过在线学习实现环境自适应导航,在开阔环境中自动降低η提高速度、在狭窄环境中自动提高η降低速度、在动态障碍物场景中自动调整d_max和d_min平衡安全与效率;针对大规模点云处理,提出两级筛选策略,第一级ROI选择基于可达性圆锥(d_reach=v_max×T×dt,动态调整θ_fov)筛选可达障碍物点,第二级方向性采样将前方180度视野划分为角度区间、每区间选择最近K个点(K=2-3),将点云规模从数千降至数十,支撑实时导航。

#### 4) 搭建基于ROS的仿真实验平台

本研究将搭建完整的仿真实验平台和真实机器人测试系统,技术路线如图5所示:

```
┌─────────────────────────────────────────────────────────────────┐
│                    硬件平台搭建                                  │
├─────────────────────────────────────────────────────────────────┤
│  • 机器人平台: 阿克曼转向运动学模型                              │
│  • 激光雷达: 10Hz扫描频率, 0.25°角度分辨率, 0.1-30m测距         │
│  • 车载计算机: Intel i7处理器, 16GB内存                          │
│  • 通信接口: 串口/CAN总线 → 底层控制器                           │
└─────────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────────┐
│                  基于ROS的软件系统构建                           │
├─────────────────────────────────────────────────────────────────┤
│  ┌─────────────┐      ┌─────────────┐      ┌─────────────┐     │
│  │ 感知模块    │ -->  │ 规划模块    │ -->  │ 控制模块    │     │
│  │ • 激光雷达  │      │ • PLAN算法  │      │ • 运动学逆解│     │
│  │ • 坐标变换  │      │ • PDPL-Net  │      │ • 轮速计算  │     │
│  │ • ROI筛选   │      │ • LNRMP     │      │ • 指令发送  │     │
│  └─────────────┘      └─────────────┘      └─────────────┘     │
│         ↓                     ↓                     ↓           │
│    点云数据            最优控制序列            轮速指令          │
│                                                                 │
│  模块通信: ROS话题 + 服务, 支持分布式部署                        │
└─────────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────────┐
│              仿真环境搭建 (IR-SIM平台)                           │
├─────────────────────────────────────────────────────────────────┤
│  场景1: 凸障碍物    → 测试基本避障能力                          │
│  场景2: 走廊        → 测试狭窄空间导航能力                      │
│  场景3: 动态障碍物  → 测试动态避障能力                          │
│  场景4: 非凸障碍物  → 测试复杂环境适应能力                      │
│  场景5: 路径跟踪    → 测试轨迹跟踪精度                          │
│  场景6: 倒车入库    → 测试倒车和精确定位能力                    │
│                                                                 │
│  每类场景设置不同难度等级 (障碍物数量、密度、速度)               │
└─────────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────────┐
│                    真实环境测试                                  │
├─────────────────────────────────────────────────────────────────┤
│  测试场景:                                                       │
│  • 室内: 办公室、实验室                                          │
│  • 室外: 广场、停车场                                            │
│                                                                 │
│  测试任务:                                                       │
│  • 点到点导航、路径跟踪、动态避障                                │
│                                                                 │
│  评估指标:                                                       │
│  • 成功率: 到达目标且无碰撞的比例                                │
│  • 路径长度: 实际路径与最短路径的比值                            │
│  • 路径平滑度: 转向角变化率的标准差                              │
│  • 计算时间: 单步规划耗时                                        │
│  • 最小障碍物距离: 轨迹上与障碍物的最小距离                      │
│                                                                 │
│  统计分析: 100+次导航任务                                        │
└─────────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────────┐
│                  对比实验与消融实验                              │
├─────────────────────────────────────────────────────────────────┤
│  对比方法:                                                       │
│  • 动态窗口法 (DWA)                                              │
│  • 时间弹性带 (TEB)                                              │
│  • 标准模型预测控制 (MPC)                                        │
│                                                                 │
│  对比指标: 成功率、路径质量、计算效率                            │
│                              ↓                                   │
│  消融实验:                                                       │
│  • PDPL-Net: 硬投影机制、可学习近端算子的独立贡献                │
│  • LNRMP: 在线学习机制、ROI采样策略的独立贡献                    │
│                                                                 │
│  输出: 性能评估报告、各创新点贡献分析                            │
└─────────────────────────────────────────────────────────────────┘
```

图5 仿真实验平台搭建与验证技术路线

本研究基于阿克曼转向运动学模型搭建研究平台,配置激光雷达传感器(10Hz扫描频率、0.25度角度分辨率、0.1-30米测距范围)和车载计算机(Intel i7处理器、16GB内存),激光雷达通过ROS驱动节点发布点云数据,车载计算机运行PLAN导航算法,通过串口或CAN总线向底层控制器发送速度和转向角指令,形成感知-规划-控制闭环;基于ROS机器人操作系统构建完整的导航系统框架,包括感知模块(订阅激光雷达点云话题、坐标变换、ROI筛选)、规划模块(运行PLAN算法、调用PDPL-Net生成对偶变量、调用LNRMP求解优化问题)和控制模块(运动学逆解、轮速计算、指令发送),各模块通过ROS话题和服务通信,支持分布式部署和模块化开发;采用IR-SIM平台(基于Python、支持二维环境建模和机器人仿真、提供激光雷达仿真和碰撞检测)构建六类典型场景(凸障碍物、走廊、动态障碍物、非凸障碍物、路径跟踪、倒车入库),每类场景设置不同难度等级(改变障碍物数量、密度、速度),全面测试算法性能;在室内办公室、实验室以及室外广场、停车场等场景进行真实环境测试,测试任务包括点到点导航、路径跟踪、动态避障,评估指标包括成功率、路径长度、路径平滑度、计算时间、最小障碍物距离,通过100+次导航任务统计分析验证算法实时性、安全性和鲁棒性;与传统方法(DWA、TEB、标准MPC)进行对比实验,对比成功率、路径质量、计算效率,定量评估PLAN算法性能优势,针对PDPL-Net和LNRMP两个核心模块进行消融实验,评估硬投影机制、可学习近端算子、在线学习机制、ROI采样策略等创新点的独立贡献,通过逐一移除各创新点并测试性能变化,量化每个创新点对整体性能的提升幅度,为后续研究提供可靠的实验依据。

### 3.4 技术路线

本研究的技术路线分为六个阶段,逐步推进算法设计、实现和验证工作。整体技术路线如图1所示:

```
┌─────────────────────────────────────────────────────────────────────────┐
│                        阶段一:平台搭建与环境配置 (3个月)                    │
├─────────────────────────────────────────────────────────────────────────┤
│  硬件平台搭建              软件环境配置              仿真环境搭建           │
│  • 差速/阿克曼机器人       • ROS导航框架            • IR-SIM平台          │
│  • 激光雷达传感器          • 激光雷达驱动            • 多场景构建          │
│  • 车载计算机              • 运动控制接口            • 可视化工具          │
└─────────────────────────────────────────────────────────────────────────┘
                                      ↓
┌─────────────────────────────────────────────────────────────────────────┐
│                      阶段二:PLAN算法架构设计 (1个月)                       │
├─────────────────────────────────────────────────────────────────────────┤
│  MPC框架设计               模块接口定义              迭代优化机制          │
│  • 预测时域设置            • IDUNE输入/输出          • 收敛条件设计        │
│  • 控制周期设置            • LNRMP输入/输出          • 最大迭代次数        │
│  • 约束条件定义            • 数据流转协议            • 实时性保证          │
└─────────────────────────────────────────────────────────────────────────┘
                                      ↓
┌─────────────────────────────────────────────────────────────────────────┐
│                    阶段三:IDUNE模块设计与实现 (3个月)                      │
├─────────────────────────────────────────────────────────────────────────┤
│  PDPL-Net网络设计          训练数据生成              模型训练与评估        │
│  • 特征编码器              • 随机场景生成            • GPU加速训练         │
│  • PDHG展开层(J步)         • 真值对偶变量求解        • 对偶可行性评估      │
│  • 硬投影层                • 10万组训练样本          • 距离估计精度        │
│  • 可学习近端算子          • KKT残差标签生成         • 推理时延测试        │
└─────────────────────────────────────────────────────────────────────────┘
                                      ↓
┌─────────────────────────────────────────────────────────────────────────┐
│                    阶段四:LNRMP模块设计与实现 (3个月)                      │
├─────────────────────────────────────────────────────────────────────────┤
│  可学习参数定义            在线学习机制              仿真测试与优化        │
│  • q_s状态权重             • 任务损失函数            • 多场景测试          │
│  • p_u控制权重             • 梯度计算与更新          • 参数收敛分析        │
│  • eta避障权重             • 自动微分机制            • 学习率调优          │
│  • d_max/d_min安全距离     • 在线学习集成            • 稳定性验证          │
└─────────────────────────────────────────────────────────────────────────┘
                                      ↓
┌─────────────────────────────────────────────────────────────────────────┐
│                  阶段五:ROI与方向性采样策略实现 (2个月)                    │
├─────────────────────────────────────────────────────────────────────────┤
│  ROI选择策略               方向性采样策略            性能优化              │
│  • 可达性圆锥计算          • 关键方向采样            • 计算复杂度分析      │
│  • 动态视野角度            • 最近邻点选择            • 实时性测试          │
│  • 最大可达距离            • 两级筛选集成            • 安全性验证          │
└─────────────────────────────────────────────────────────────────────────┘
                                      ↓
┌─────────────────────────────────────────────────────────────────────────┐
│                    阶段六:多场景实验验证 (3个月)                          │
├─────────────────────────────────────────────────────────────────────────┤
│  仿真环境测试              真实环境测试              对比与消融实验        │
│  • 凸障碍物场景            • 室内办公室/实验室       • DWA/TEB/MPC对比     │
│  • 走廊场景                • 室外广场/停车场         • IDUNE消融实验       │
│  • 动态障碍物场景          • 成功率/安全性测试       • LNRMP消融实验       │
│  • 非凸障碍物场景          • 实时性/鲁棒性验证       • 各创新点贡献评估    │
│  • 路径跟踪/倒车入库       • 100+次导航任务          • 论文撰写与开源      │
└─────────────────────────────────────────────────────────────────────────┘

图1 PLAN算法技术路线图
```

**阶段一:平台搭建与环境配置**。硬件方面,选择差速驱动或阿克曼转向机器人作为实验平台,配置激光雷达传感器用于环境感知,配置车载计算机用于算法运行。软件方面,基于ROS构建导航系统框架,集成激光雷达驱动、运动控制接口和可视化工具。同时搭建仿真环境,采用IR-SIM平台构建多种测试场景,为后续算法开发和测试提供支撑。

**阶段二:PLAN算法架构设计**。在模型预测控制框架下,定义IDUNE和LNRMP两个模块的功能边界和数据接口。IDUNE模块接收障碍物点云数据,输出对偶变量和距离特征。LNRMP模块接收对偶变量和距离特征,输出最优轨迹和控制指令。两个模块通过迭代优化实现协同工作,每次迭代中IDUNE根据当前轨迹更新对偶变量,LNRMP根据新的对偶变量优化轨迹,直到满足收敛条件或达到最大迭代次数。

**阶段三:IDUNE模块设计与实现**。核心工作是开发PDPL-Net前端,将原始对偶混合梯度算法展开为神经网络层。网络结构包括特征编码器、对偶变量初始化层、PDHG展开层和硬投影层。特征编码器采用多层感知机,将点云坐标映射为高维特征。对偶变量初始化层根据特征生成初始的对偶变量和辅助变量。PDHG展开层执行若干步迭代优化,每步包含对偶更新、原始更新和可学习近端算子。硬投影层保证输出的对偶变量满足非负性和范数约束。训练数据通过随机生成机器人位置和障碍物点云,求解原始优化问题获得真值对偶变量。训练目标包括对偶变量的均方误差、距离估计的均方误差以及KKT残差正则化项。

**阶段四:LNRMP模块设计与实现**。核心工作是将优化问题中的关键参数转化为可学习变量,并开发在线学习机制。可学习参数包括状态跟踪权重、控制平滑权重、避障成本权重以及安全距离上下限。这些参数在初始化时设置为合理的默认值,在实际导航过程中根据任务反馈自动调整。任务损失函数综合考虑多个因素,包括机器人是否到达目标点、是否发生碰撞、路径是否平滑、控制输入是否剧烈变化等。通过自动微分机制计算损失函数对可学习参数的梯度,使用梯度下降方法更新参数。在线学习过程与导航任务同步进行,每执行一步导航就更新一次参数,使优化器逐步适应当前环境特性。

**阶段五:ROI与方向性采样策略实现**。ROI选择基于可达性圆锥,根据机器人的预测时域和最大速度计算最大可达距离,根据路径曲率动态调整视野角度,筛选出对当前规划任务相关的障碍物点。方向性采样针对机器人前进方向的关键角度进行定向采样,提取窗口内的点,结合最近邻策略选择距离机器人最近的威胁点。两级筛选策略将点云规模从数千降低至数十,大幅降低计算负担,同时保证不丢失关键障碍物信息。

**阶段六:多场景实验验证**。仿真实验在IR-SIM平台进行,构建凸障碍物、走廊、动态障碍物、非凸障碍物、路径跟踪、倒车入库等多种场景,测试算法在不同复杂度环境下的性能表现。真实环境测试在室内办公室、实验室以及室外广场、停车场等场景进行,验证算法的实时性、安全性和鲁棒性。评估指标包括成功率、路径长度、路径平滑度、计算时间、最小障碍物距离等。与传统方法如动态窗口法、时间弹性带以及标准模型预测控制进行对比分析,定量评估本文方法的优势。此外,针对IDUNE和LNRMP两个核心模块分别设计消融实验,定量评估各创新点的独立贡献。

### 3.5 可行性分析

本研究的可行性体现在理论基础、技术实现和实验条件三个方面。

从理论基础来看,本研究所采用的核心技术均有坚实的理论支撑。模型预测控制是成熟的控制理论,在工业界和学术界有广泛应用。凸优化理论保证了LNRMP模块构建的优化问题存在全局最优解,可通过成熟的求解器高效求解。原始对偶混合梯度算法的收敛性已被严格证明,展开后的神经网络继承了这一性质。可微优化层技术实现了神经网络与凸优化的无缝集成,支持端到端训练。这些理论基础为本研究提供了可靠的理论保证。

从技术实现来看,本研究所需的关键技术均已具备实现条件。PyTorch深度学习框架提供了强大的自动微分和神经网络构建能力,支持IDUNE模块的开发和训练。CVXPY凸优化建模库和CvxpyLayers可微优化层库支持LNRMP模块的实现。ROS机器人操作系统提供了完整的导航系统框架,支持激光雷达集成和运动控制。IR-SIM仿真平台提供了丰富的测试场景,支持算法的快速迭代和验证。这些工具和平台为本研究提供了完备的技术支撑。

从实验条件来看,本研究所需的硬件设备和软件环境均已具备。实验室配备了差速驱动和阿克曼转向机器人平台,配置了激光雷达传感器和车载计算机。计算资源方面,训练IDUNE模块需要GPU加速,实验室配备了NVIDIA GPU服务器,满足训练需求。推理阶段仅需CPU即可满足实时性要求,车载计算机的性能足以支撑算法运行。场地条件方面,实验室具备室内测试场地,可搭建各种障碍物场景进行测试。这些实验条件为本研究提供了充分的保障。

综上所述,本研究在理论基础、技术实现和实验条件三个方面均具备可行性,能够顺利开展并完成预期目标。

---

## 4. 论文研究的特色与创新之处

本研究的特色与创新主要体现在四个方面:可解释的神经编码器设计、可学习的运动规划器开发、高效的点云处理策略以及完整的系统实现与验证。

**创新点一:可解释的原始对偶近端学习网络PDPL-Net**。传统端到端学习方法采用黑盒神经网络,缺乏可解释性且难以保证输出满足物理约束。本研究提出PDPL-Net,将经典的原始对偶混合梯度算法展开为可学习的神经网络层,使网络的计算过程具有明确的物理意义。每个展开步骤包含对偶更新、原始更新和可行性投影三个操作,对应优化算法的迭代过程。通过引入零参数硬投影机制,严格保证输出的对偶变量满足非负性和范数约束,将违反率从百分之十降低至百分之一以下。同时在展开步骤中引入可学习的近端算子,通过轻量级神经网络学习残差修正,在保持约束可行性的前提下提升特征表达能力。这种设计兼顾了优化理论的严谨性和神经网络的学习能力,为约束优化与深度学习的融合提供了新的技术范式。该创新点首次将原始对偶混合梯度算法展开应用于点云到对偶变量的映射,连接了约束神经场、展开优化等前沿研究方向,具有重要的学术价值。

**创新点二:基于在线学习的可学习参数优化框架**。传统凸优化求解器中的参数由人工设定,存在场景依赖性强、调参成本高、泛化能力弱等问题。本研究提出基于在线学习的可学习参数优化框架,将优化问题中的关键参数转化为可学习变量,通过梯度下降方法根据任务损失自动调整。任务损失综合考虑机器人是否成功到达目标、是否发生碰撞、路径是否平滑等多个因素,通过自动微分机制计算梯度,使用梯度下降方法更新参数。这种自适应机制在实际导航过程中在线学习,无需离线训练数据,能够自动适应不同场景,无需人工调参。通过学习更优的参数配置,还能加速凸优化求解过程,减少迭代次数,降低计算时延。该创新点将端到端学习思想引入凸优化求解器,实现感知-规划-控制全流程的可学习化,为学习型模型预测控制提供了新的技术思路,具有重要的工程价值和应用前景。

**创新点三:融合ROI与方向性采样的高效点云处理策略**。移动机器人在复杂环境中可能面临大规模点云数据,直接处理会导致计算负担过重。本研究提出基于可达性圆锥的ROI选择策略和方向性增强采样机制,实现两级点云筛选。第一级根据机器人的预测时域和动力学约束,计算最大可达距离和视野角度,筛选出对当前规划任务真正相关的障碍物点。第二级针对机器人前进方向的关键角度进行定向采样,结合最近邻点选择,确保在降低计算量的同时不丢失关键障碍物信息。视野角度根据路径曲率自适应调整,直线路径采用较窄视野以提高效率,弯曲路径扩大视野以保证安全。通过这种智能筛选策略,将点云规模从数千降低至数十,将神经编码器的推理时延从数毫秒降低至亚毫秒级别,总计算时间控制在二十毫秒以内,满足五十赫兹控制频率的实时性要求。该创新点为大规模点云实时处理提供了一种高效且安全的解决方案,可推广至自动驾驶、无人机等领域。

**创新点四:完整的系统实现与开源贡献**。本研究不仅提出了理论创新,还提供了完整的系统实现和开源代码,具有很强的可复现性和实用性。基于PyTorch和CVXPY构建了完整的PLAN算法框架,基于ROS开发了导航系统集成包,支持真实机器人部署。在仿真和真实环境中进行了全面的实验验证,建立了完整的评测体系,包括模块级评测和闭环评测。模块级评测针对IDUNE模块,评估对偶可行性违反率、距离估计精度、推理时延等指标。闭环评测针对整个导航系统,评估成功率、路径长度、路径平滑度、计算时间、最小障碍物距离等指标。通过与传统方法的对比分析,定量评估本文方法的优势。通过消融实验,定量评估各创新点的独立贡献。这些工作为学术界和工业界提供了一个高质量的基准方法,推动了端到端导航技术的发展和应用。

---

## 5. 预期成果与研究计划

### 5.1 预期成果

本研究预期在理论、技术和应用三个层面取得成果。

理论成果方面,提出完整的PLAN算法框架及其理论分析,证明近端交替最小化算法的收敛性和PDPL-Net的对偶可行性保证,建立端到端导航的误差分析理论。发表高水平学术论文,将IDUNE模块的创新点投稿至机器人领域顶级会议如ICRA或IROS,将LNRMP模块的创新点投稿至机器学习领域顶级会议如NeurIPS或ICML,将完整系统的工作投稿至机器人领域顶级期刊如IEEE Transactions on Robotics。

技术成果方面,开发完整的PLAN导航系统,包括IDUNE模块、LNRMP模块、ROI选择模块和ROS集成包。训练高性能的PDPL-Net模型,实现对偶可行性违反率低于百分之一、距离估计误差降低百分之十五以上、推理时延控制在两毫秒以内。实现可学习参数优化框架,支持在线学习和自适应调整,提升系统对不同场景的泛化能力。

应用成果方面,在仿真环境中完成十种以上典型场景的测试,验证算法在不同复杂度环境下的性能表现。在真实机器人平台上完成一百次以上导航任务,验证算法的实时性、安全性和鲁棒性。与五种以上传统方法进行定量对比,证明本文方法在成功率、路径质量、计算效率等指标上的优势。开源完整的代码和预训练模型,为学术界和工业界提供可复现的基准方法。

### 5.2 研究计划

本研究计划分为五个阶段,总计十五个月完成。

第一阶段为前期准备,计划用时三个月。主要任务包括文献调研、理论分析、平台搭建和环境配置。文献调研方面,系统梳理移动机器人导航、端到端学习、凸优化、展开优化等相关领域的研究进展,明确本研究的创新点和技术路线。理论分析方面,推导PLAN算法的收敛性条件,分析PDPL-Net的对偶可行性保证,建立误差传播的理论模型。平台搭建方面,完成机器人硬件系统集成,配置激光雷达传感器和车载计算机,搭建ROS导航系统框架。环境配置方面,搭建仿真环境,构建多种测试场景,为后续算法开发和测试提供支撑。

第二阶段为IDUNE模块开发,计划用时三个月。主要任务包括PDPL-Net网络设计、训练数据生成、模型训练和性能评估。网络设计方面,实现特征编码器、对偶变量初始化层、PDHG展开层和硬投影层,确定网络结构和超参数。训练数据生成方面,随机生成机器人位置和障碍物点云,求解原始优化问题获得真值对偶变量,生成十万组训练样本。模型训练方面,使用GPU加速训练,采用Adam优化器,设置学习率衰减策略,训练两千五百轮。性能评估方面,在测试集上评估对偶可行性违反率、距离估计精度、推理时延等指标,与基线方法进行对比分析。

第三阶段为LNRMP模块开发,计划用时三个月。主要任务包括可学习参数定义、在线学习机制开发、仿真测试和性能优化。可学习参数定义方面,将状态跟踪权重、控制平滑权重、避障成本权重以及安全距离上下限转化为可学习变量,设置合理的初始值和约束范围。在线学习机制开发方面,定义任务损失函数,实现梯度计算和参数更新,集成到导航系统中。仿真测试方面,在多种场景中测试在线学习效果,观察参数的收敛过程和最终值。性能优化方面,调整学习率、损失函数权重等超参数,优化在线学习的稳定性和收敛速度。

第四阶段为系统集成与仿真验证,计划用时三个月。主要任务包括ROI选择和方向性采样实现、PLAN算法集成、多场景仿真测试和对比分析。ROI选择和方向性采样实现方面,开发可达性圆锥筛选算法,实现动态视野角度调整,开发方向性采样和最近邻选择策略。PLAN算法集成方面,将IDUNE模块、LNRMP模块和ROI选择模块集成为完整的导航系统,实现模块间的数据流转和迭代优化。多场景仿真测试方面,在凸障碍物、走廊、动态障碍物、非凸障碍物、路径跟踪、倒车入库等场景中测试算法性能。对比分析方面,与动态窗口法、时间弹性带、标准模型预测控制等传统方法进行定量对比,评估本文方法的优势。

第五阶段为实物部署与论文撰写,计划用时三个月。主要任务包括真实环境测试、消融实验、论文撰写和开源发布。真实环境测试方面,将算法部署到真实机器人平台,在室内办公室、实验室以及室外广场、停车场等场景进行测试,验证算法的实时性、安全性和鲁棒性。消融实验方面,针对IDUNE和LNRMP两个核心模块分别设计消融实验,定量评估各创新点的独立贡献。论文撰写方面,整理研究成果,撰写学术论文,投稿至顶级会议和期刊。开源发布方面,整理代码和文档,发布到GitHub等开源平台,为学术界和工业界提供可复现的基准方法。

---

## 参考文献

[1] Han R, Wang S, Wang S, et al. NeuPAN: Direct Point Robot Navigation with End-to-End Model-based Learning[J]. IEEE Transactions on Robotics, 2025.

[2] Chambolle A, Pock T. A first-order primal-dual algorithm for convex problems with applications to imaging[J]. Journal of mathematical imaging and vision, 2011, 40: 120-145.

[3] Agrawal A, Amos B, Barratt S, et al. Differentiable convex optimization layers[J]. Advances in neural information processing systems, 2019, 32.

[4] Fox D, Burgard W, Thrun S. The dynamic window approach to collision avoidance[J]. IEEE Robotics & Automation Magazine, 1997, 4(1): 23-33.

[5] Rösmann C, Feiten W, Wösch T, et al. Trajectory modification considering dynamic constraints of autonomous robots[C]//ROBOTIK 2012; 7th German Conference on Robotics. VDE, 2012: 1-6.

