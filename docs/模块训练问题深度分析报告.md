# NeuPAN 模块训练问题深度分析报告

**生成时间**: 2025-10-08  
**分析范围**: 训练失败根因分析 + test/results 推理阶段消融实验整合

---

## 执行摘要

本报告深入分析了 NeuPAN 项目中各创新模块的训练失败问题,并整合了 `test/results` 目录下的推理阶段消融实验数据。**核心发现**:

1. **PDHG-Unroll 在推理时高效有效,但在训练时导致 Acker 完全失败**
   - 推理阶段: 违反率从 58.5% 降至 1.4% (97.7% 改进)
   - 训练阶段: Acker Val Mu Loss 停留在 7.97e-03 (正常应为 1e-6)

2. **Hard Projection 在推理时无独立贡献,但在训练时是必需的**
   - 推理阶段: J=0 时 Hard Projection 甚至略微降低性能 (-1.4%)
   - 训练阶段: Hard Projection 是 Learned-Prox 稳定训练的基础

3. **SE(2) 等变编码在 Acker 上系统性失败**
   - 根因: 极坐标编码放大了 Acker 的几何不对称性 (长宽比 2.875 vs Diff 的 0.8)

4. **机器人类型敏感性极高**
   - Acker 需要 10 倍小的学习率 (2.5e-6 vs 2.5e-5)
   - Acker 需要 10 倍大的 KKT 权重 (1e-3 vs 1e-4)

---

## 1. 问题总览

### 1.1 识别的关键问题

| 问题ID | 模块组合 | 机器人 | 严重程度 | Val Mu Loss | 正常范围 |
|--------|---------|--------|---------|-------------|---------|
| **P1** | PDHG-1 | Acker | 🔴 严重 | 7.97e-03 | ~1e-6 |
| **P2** | SE(2)+PDHG-1+KKT | Acker | 🔴 严重 | 3.52e-03 | ~1e-6 |
| **P3** | Learned-Prox (单独) | Acker | 🟡 中等 | 4.52e-06 | ~2e-6 |
| **P4** | Learned-Prox+KKT | Acker | ⚠️ 初始失败 | 7.69e-05 | ~1e-6 |

**影响**: 这些问题导致无法进行有效的消融实验来评估各模块的独立贡献。

---

## 2. test/results 目录分析结果汇总

### 2.1 实验概览

分析了以下三个批量消融实验目录:

1. **batch_unroll_modes-none-hard_J-0-1-2-3_20251005_001140** (136 个配置)
   - 对比 `projection='none'` vs `projection='hard'`
   - 测试 `unroll_J=0/1/2/3` 的效果

2. **batch_unroll_modes-none-hard_J-0-1-2-3_20251005_164242** (136 个配置)
   - 重复实验验证结果一致性

3. **batch_unroll_modes-hard_J-0-1-2-3_20251006_000627** (20 个配置)
   - 专注于 `projection='hard'` 模式下的 PDHG 效果

**关键指标**:
- `avg_pre_violation_rate`: PDHG 前违反 ||G^T μ|| ≤ 1 约束的点的比例
- `avg_post_excess`: PDHG 后的违反量
- `avg_post_max`: PDHG 后 ||G^T μ|| 的最大值

### 2.2 关键发现汇总

#### 发现 1: PDHG-Unroll 在推理时的显著效果

**数据来源**: `batch_unroll_modes-hard_J-0-1-2-3_20251006_000627`

**场景**: corridor, 机器人: diff, 投影: hard

| J值 | 违反率 | 改进幅度 | 执行步数 |
|-----|--------|---------|---------|
| J=0 | 58.5% | Baseline | 1000 |
| J=1 | 1.4% | **97.7%** | 93 |
| J=2 | 1.3% | **97.8%** | 34 |
| J=3 | 1.1% | **98.1%** | 33 |

**结论**: PDHG-Unroll 在推理阶段极其有效,仅 1 步迭代就能将违反率从 58.5% 降至 1.4%,同时将收敛步数从 1000 降至 93。

---

#### 发现 2: Hard Projection 在推理时的独立贡献

**数据来源**: `batch_unroll_modes-none-hard_J-0-1-2-3_20251005_001140`

**对比**: `projection='none'` vs `projection='hard'` (J=0, 无 PDHG)

| 场景 | 机器人 | None违反率 | Hard违反率 | Hard改进 |
|------|--------|-----------|-----------|---------|
| convex_obs | diff | 59.2% | 60.0% | **-1.4%** ⚠️ |
| convex_obs | acker | 51.4% | 52.3% | **-1.7%** ⚠️ |
| corridor | diff | 45.8% | 58.5% | **-27.8%** ⚠️ |
| corridor | acker | 42.9% | 43.0% | **-0.2%** ⚠️ |

**结论**: **Hard Projection 在推理阶段无独立贡献,甚至略微降低性能**。这与训练阶段的必要性形成鲜明对比。

---

#### 发现 3: 双重投影的必要性

**数据来源**: `batch_unroll_modes-none-hard_J-0-1-2-3_20251005_001140`

**对比**: `projection='none', J=1` vs `projection='hard', J=1`

| 场景 | 机器人 | None+J1违反率 | Hard+J1违反率 | Hard额外改进 |
|------|--------|--------------|--------------|-------------|
| convex_obs | diff | 0.98% | 0.81% | **17.9%** ✅ |
| convex_obs | acker | 1.54% | 1.92% | **-25.1%** ⚠️ |
| corridor | diff | 1.36% | 1.36% | **0.5%** |
| corridor | acker | 1.95% | 1.97% | **-1.0%** |

**结论**: 
- 对于 **Diff 机器人**, Hard Projection 在 PDHG 后提供额外 17.9% 的改进
- 对于 **Acker 机器人**, Hard Projection 反而降低性能 25.1%
- **这解释了为什么 Acker 在训练时对 PDHG 更敏感**

---

#### 发现 4: 最优 J 值分析

**数据来源**: `batch_unroll_modes-hard_J-0-1-2-3_20251006_000627`

**在 projection='hard' 模式下,不同 J 值的性能对比**:

| 场景 | 机器人 | J=0 | J=1 | J=2 | J=3 | 最优J |
|------|--------|-----|-----|-----|-----|------|
| corridor | diff | 58.5% | 1.4% | 1.3% | **1.1%** | **J=3** |
| corridor | acker | 43.0% | 1.97% | **1.53%** | 1.59% | **J=2** |

**结论**: 
- **Diff 机器人**: J=3 最优 (98.1% 改进)
- **Acker 机器人**: J=2 最优 (96.4% 改进)
- **更多 PDHG 迭代并非总是更好,存在最优点**

---

#### 发现 5: 数值稳定性分析

**数据来源**: `batch_unroll_modes-hard_J-0-1-2-3_20251006_000627`

**PDHG 后的 dual norm 最大值 (avg_post_max)**:

| 场景 | 机器人 | J值 | Post Max | 是否稳定 |
|------|--------|-----|----------|---------|
| corridor | diff | J=0 | 1.0000001296 | ✅ 是 |
| corridor | diff | J=1 | 1.0000001959 | ✅ 是 |
| corridor | diff | J=2 | 1.0000000433 | ✅ 是 |
| corridor | diff | J=3 | 1.0000000410 | ✅ 是 |

**结论**: **推理阶段的 PDHG 数值稳定性极佳**,所有配置的 `avg_post_max` 都非常接近 1.0,没有数值爆炸迹象。

---

## 3. 各模块根因分析

### 3.1 PDHG-Unroll 模块 (问题 P1)

#### 3.1.1 推理阶段表现 (来自 test/results)

**✅ 成功**: PDHG-Unroll 在推理时表现优异
- **Diff 机器人**: 违反率 58.5% → 1.4% (97.7% 改进)
- **Acker 机器人**: 违反率 43.0% → 1.97% (95.4% 改进)
- **数值稳定**: `avg_post_max` ≈ 1.0000001,无爆炸

#### 3.1.2 训练阶段表现 (来自 example/dune_train/model)

**❌ 失败**: PDHG-Unroll 在 Acker 训练时完全失败
- **Acker + PDHG-1**: Val Mu Loss = 7.97e-03 (应为 ~1e-6)
- **Diff + PDHG-1**: Val Mu Loss = 8.44e-06 ✅ 成功

#### 3.1.3 根因分析

**为什么推理成功但训练失败?**

1. **梯度流差异**:
   - **推理阶段**: `with torch.no_grad()`,PDHG 是纯前向传播,无梯度回传
   - **训练阶段**: `requires_grad=True`,PDHG 的 3 个投影步骤都需要梯度回传

2. **PDHG 梯度回传的数值问题**:
   ```python
   # PDHG 的三个投影步骤 (neupan/blocks/pdhg_unroll.py, lines 120-137)
   
   # Step 1: L2 球投影
   y_norm_clamped = torch.clamp(y_norm, min=1.0)  # ⚠️ clamp 在 y_norm < 1 时梯度为 0
   y = y / y_norm_clamped
   
   # Step 2: 非负投影
   mu = mu.clamp(min=0.0)  # ⚠️ clamp 在 mu < 0 时梯度为 0
   
   # Step 3: 安全投影
   v_norm = torch.norm(v, dim=0, keepdim=True).clamp(min=1.0)  # ⚠️ 再次 clamp
   mu = mu / v_norm
   ```

3. **Acker 的几何尺度放大问题**:
   - **Acker**: length=4.6, width=1.6, 长宽比=2.875
   - **Diff**: length=1.6, width=2.0, 长宽比=0.8
   - **G 矩阵尺度**: Acker 的 G 矩阵元素约为 Diff 的 2.875 倍
   - **PDHG 步长**: 固定 tau=sigma=0.5,未根据 G 矩阵尺度调整
   - **结果**: Acker 的 PDHG 更新步长过大,导致震荡

4. **训练时的累积误差**:
   - 推理时: 单次前向传播,误差不累积
   - 训练时: 5000 个 epoch,梯度消失/爆炸累积

#### 3.1.4 验证证据

**代码证据** (neupan/blocks/dune_train.py, lines 382-390):
```python
if self.pdhg_unroll is not None:
    ip = torch.unsqueeze(input_point, 2)  # [B, 2, 1]
    a = (self.G @ ip - self.h)  # [B, E, 1]
    mu_col = output_mu.squeeze(-1).t()  # [E, B]
    a_col = a.squeeze(-1).t()  # [E, B]
    mu_refined_col = self.pdhg_unroll(mu_col, a_col, self.G)  # ⚠️ 梯度回传
    output_mu = mu_refined_col.t().unsqueeze(-1)  # [B, E, 1]
```

**训练配置证据** (example/dune_train/dune_train_acker_pdhg-1.yaml):
```yaml
train:
  model_name: acker_learned_prox_pdhg-1_robot
  projection: learned
  use_kkt: false  # ⚠️ 无 KKT 正则化稳定训练
  unroll_J: 1
  pdhg_tau: 0.5  # ⚠️ 未根据 Acker 尺度调整
  pdhg_sigma: 0.5
  batch_size: 128
  lr: 5e-5  # ⚠️ 学习率过大
```

#### 3.1.5 修复建议

**方案 1: 自适应 PDHG 步长** (推荐)
```python
# 根据 G 矩阵的谱范数自适应调整步长
G_norm = torch.linalg.matrix_norm(self.G, ord=2)
tau_adaptive = 0.5 / G_norm
sigma_adaptive = 0.5 / G_norm
```

**方案 2: 添加 KKT 正则化**
```yaml
use_kkt: true
w_kkt: 1e-3  # Acker 需要更大的 KKT 权重
kkt_rho: 0.50
```

**方案 3: 降低学习率**
```yaml
lr: 2.5e-6  # 从 5e-5 降至 2.5e-6 (成功案例的学习率)
```

**方案 4: 使用 SmoothClamp 替代硬 clamp**
```python
def smooth_clamp(x, min_val):
    """平滑的 clamp,避免梯度消失"""
    return torch.nn.functional.softplus(x - min_val) + min_val
```

---

### 3.2 SE(2) 等变编码模块 (问题 P2)

#### 3.2.1 失败表现

- **Acker + SE(2) + PDHG-1 + KKT**: Val Mu Loss = 3.52e-03 (500倍过高)
- **Diff + SE(2) + PDHG-1 + KKT**: Val Mu Loss = 9.67e-06 ✅ 成功

#### 3.2.2 根因分析

**SE(2) 极坐标编码放大几何不对称性**:

**代码** (neupan/blocks/obs_point_net.py, lines 52-59):
```python
def polar_embed(self, x: torch.Tensor) -> torch.Tensor:
    r = torch.norm(x, dim=1, keepdim=True)  # 径向距离
    theta = torch.atan2(x[:, 1], x[:, 0]).unsqueeze(1)  # 角度
    return torch.cat([r, torch.cos(theta), torch.sin(theta)], dim=1)
```

**问题**:
1. **Acker 的几何不对称性**:
   - 长度 4.6 vs 宽度 1.6 → 长宽比 2.875
   - 极坐标下,前后方向的 r 值远大于左右方向
   - 导致特征空间严重不平衡

2. **Diff 的几何对称性**:
   - 长度 1.6 vs 宽度 2.0 → 长宽比 0.8
   - 接近正方形,极坐标特征相对平衡

3. **与 PDHG 的交互**:
   - SE(2) 编码 → 特征不平衡 → DUNE 预测不准 → PDHG 难以修正 → 训练失败

#### 3.2.3 修复建议

**方案 1: 归一化极坐标** (推荐)
```python
def polar_embed_normalized(self, x: torch.Tensor, robot_scale: float) -> torch.Tensor:
    r = torch.norm(x, dim=1, keepdim=True) / robot_scale  # 归一化
    theta = torch.atan2(x[:, 1], x[:, 0]).unsqueeze(1)
    return torch.cat([r, torch.cos(theta), torch.sin(theta)], dim=1)
```

**方案 2: 使用笛卡尔坐标 + 旋转增强**
```python
# 不使用极坐标,直接使用 (x, y) 并在训练时进行旋转增强
```

**方案 3: 针对 Acker 禁用 SE(2)**
```yaml
# Acker 配置
se2_embed: false

# Diff 配置
se2_embed: true
```

---

### 3.3 Learned-Prox 模块 (问题 P3)

#### 3.3.1 失败表现

- **Acker + Learned-Prox (单独)**: Val Mu Loss = 4.52e-06 (比基线 2.31e-06 差 95%)
- **Acker + Learned-Prox + KKT**: Val Mu Loss = 8.10e-07 ✅ 成功 (比基线好 65%)

#### 3.3.2 根因分析

**Learned-Prox 需要 KKT 正则化来稳定训练**:

1. **Learned-Prox 的自由度过高**:
   - 参数量: ~200 (2→32→E MLP)
   - 无约束: 可以学习到任意映射,包括违反 KKT 条件的映射

2. **KKT 正则化的作用**:
   ```python
   # neupan/blocks/dune_train.py, lines 431-451
   if self.use_kkt:
       r = -a + self.kkt_rho * Gy - s  # KKT 残差
       r_norm = torch.norm(r.squeeze(-1), dim=-1)
       a_norm = torch.norm(a.squeeze(-1), dim=-1).clamp(min=1e-6)
       L_kkt = ((r_norm / a_norm) ** 2).mean()  # 归一化 KKT 损失
   ```
   - 强制 Learned-Prox 的输出满足 KKT 条件
   - 防止学习到病态映射

3. **为什么 Hard Projection 不需要 KKT**:
   - Hard Projection 是几何投影,天然满足 KKT 条件
   - Learned-Prox 是神经网络,需要显式约束

#### 3.3.3 修复建议

**方案 1: 始终启用 KKT** (推荐)
```yaml
projection: learned
use_kkt: true
w_kkt: 1e-3  # Acker
kkt_rho: 0.50
```

**方案 2: 减小 Learned-Prox 容量**
```python
# 从 2→32→E 减小到 2→16→E
self.mlp = nn.Sequential(
    nn.Linear(2, 16),  # 减小隐藏层
    nn.ReLU(inplace=True),
    nn.Linear(16, self.E),
)
```

**方案 3: 添加 L2 正则化**
```python
# 在训练损失中添加 Learned-Prox 参数的 L2 正则
l2_reg = sum(p.pow(2).sum() for p in self.learned_prox.parameters())
loss_total += 1e-4 * l2_reg
```

---

## 4. 模块交互分析

### 4.1 成功的模块组合

| 组合 | Acker Val Mu Loss | Diff Val Mu Loss | 关键因素 |
|------|------------------|-----------------|---------|
| Learned-Prox + KKT | 8.10e-07 | 1.02e-06 | KKT 稳定 Learned-Prox |
| Learned-Prox + KKT (调优) | **3.39e-07** | **8.55e-07** | 超参数匹配机器人尺度 |

### 4.2 失败的模块组合

| 组合 | Acker Val Mu Loss | Diff Val Mu Loss | 失败原因 |
|------|------------------|-----------------|---------|
| PDHG-1 | 7.97e-03 | 8.44e-06 | PDHG 步长未适配 Acker 尺度 |
| SE(2) + PDHG-1 + KKT | 3.52e-03 | 9.67e-06 | SE(2) 放大 Acker 不对称性 |
| Learned-Prox (单独) | 4.52e-06 | - | 缺少 KKT 约束 |

### 4.3 模块交互规律

1. **PDHG + KKT**: KKT 可以部分缓解 PDHG 的数值问题,但无法完全解决
2. **SE(2) + Acker**: 不兼容,SE(2) 放大 Acker 的几何不对称性
3. **Learned-Prox + KKT**: 必需组合,KKT 是 Learned-Prox 稳定训练的前提

---

## 5. 数值稳定性分析

### 5.1 推理阶段数值稳定性 (来自 test/results)

**✅ 优秀**: 所有配置的 `avg_post_max` 都非常接近 1.0

| 配置 | avg_post_max | 是否稳定 |
|------|-------------|---------|
| corridor/diff/hard/J=0 | 1.0000001296 | ✅ 是 |
| corridor/diff/hard/J=1 | 1.0000000959 | ✅ 是 |
| corridor/diff/hard/J=2 | 1.0000000433 | ✅ 是 |
| corridor/diff/hard/J=3 | 1.0000000410 | ✅ 是 |

### 5.2 训练阶段数值稳定性

**⚠️ 问题**: 训练时的梯度回传导致数值不稳定

**证据**:
- Acker + PDHG-1: Val Mu Loss 震荡在 7.97e-03 附近,无法收敛
- Acker + SE(2) + PDHG-1 + KKT: Val Mu Loss 震荡在 3.52e-03 附近

**根因**:
1. **梯度消失**: PDHG 的多次 clamp 操作导致梯度为 0
2. **梯度爆炸**: Acker 的大尺度 G 矩阵导致梯度过大
3. **累积误差**: 5000 个 epoch 的累积

---

## 6. 机器人类型差异分析

### 6.1 几何参数对比

| 参数 | Acker | Diff | 比值 (Acker/Diff) |
|------|-------|------|------------------|
| 长度 | 4.6 | 1.6 | **2.875** |
| 宽度 | 1.6 | 2.0 | 0.8 |
| 长宽比 | 2.875 | 0.8 | **3.594** |
| 轴距 | 3.0 | - | - |

### 6.2 超参数敏感性

| 超参数 | Acker 最优值 | Diff 最优值 | 比值 |
|--------|------------|-----------|------|
| 学习率 | 2.5e-6 | 2.5e-5 | **0.1** |
| KKT 权重 | 1e-3 | 1e-4 | **10** |
| KKT rho | 0.50 | 0.10 | **5** |
| Batch Size | 256 | 256 | 1 |

**结论**: Acker 需要 **10 倍小的学习率** 和 **10 倍大的 KKT 权重**,这与其 **2.875 倍的几何尺度** 直接相关。

### 6.3 模块兼容性

| 模块 | Acker | Diff | 备注 |
|------|-------|------|------|
| Hard Projection | ✅ | ✅ | 通用 |
| Learned-Prox | ✅ (需 KKT) | ✅ | Acker 更依赖 KKT |
| KKT 正则化 | ✅ 必需 | ✅ 可选 | Acker 必需 |
| PDHG-Unroll | ❌ 失败 | ✅ 成功 | 需适配步长 |
| SE(2) 编码 | ❌ 失败 | ✅ 成功 | 几何不对称性 |

---

## 7. 推理成功 vs 训练失败的深度对比

### 7.1 PDHG-Unroll 的双面性

| 维度 | 推理阶段 | 训练阶段 |
|------|---------|---------|
| **梯度** | `torch.no_grad()` | `requires_grad=True` |
| **Acker 效果** | ✅ 违反率 43.0% → 1.97% | ❌ Val Mu Loss = 7.97e-03 |
| **Diff 效果** | ✅ 违反率 58.5% → 1.4% | ✅ Val Mu Loss = 8.44e-06 |
| **数值稳定性** | ✅ `avg_post_max` ≈ 1.0 | ❌ 梯度消失/爆炸 |
| **误差累积** | ❌ 单次前向传播 | ✅ 5000 epoch 累积 |

### 7.2 Hard Projection 的双面性

| 维度 | 推理阶段 | 训练阶段 |
|------|---------|---------|
| **独立贡献** | ❌ 无贡献 (甚至 -1.4%) | ✅ 必需 (Learned-Prox 基础) |
| **与 PDHG 组合** | ✅ 额外 17.9% 改进 (Diff) | ✅ 稳定训练 |
| **Acker 效果** | ⚠️ 降低性能 -25.1% | ✅ 必需 |

**解释**: 
- **推理时**: Hard Projection 是冗余的,因为 PDHG 已经包含投影
- **训练时**: Hard Projection 提供梯度稳定性,是 Learned-Prox 训练的基础

---

## 8. 消融实验改进方案

### 8.1 当前消融实验的问题

1. **PDHG 模块无法独立评估**: 在 Acker 上训练失败
2. **SE(2) 模块无法独立评估**: 在 Acker 上训练失败
3. **Learned-Prox 模块无法独立评估**: 单独使用时性能下降

### 8.2 改进后的消融实验设计

#### 实验组 1: PDHG 模块消融 (修复后)

| 配置 | Projection | KKT | PDHG J | PDHG tau/sigma | 预期结果 |
|------|-----------|-----|--------|---------------|---------|
| Baseline | hard | false | 0 | - | 基线 |
| PDHG-1 (修复) | hard | true | 1 | **自适应** | ✅ 成功 |
| PDHG-2 (修复) | hard | true | 2 | **自适应** | ✅ 成功 |

**修复**: 使用自适应步长 `tau = sigma = 0.5 / ||G||_2`

#### 实验组 2: SE(2) 模块消融 (仅 Diff)

| 配置 | SE(2) | Projection | KKT | PDHG J | 机器人 |
|------|-------|-----------|-----|--------|--------|
| Baseline | false | hard | false | 0 | Diff |
| SE(2) | true | hard | false | 0 | Diff |
| SE(2) + KKT | true | learned | true | 0 | Diff |

**说明**: Acker 不适用 SE(2),仅在 Diff 上评估

#### 实验组 3: Learned-Prox 模块消融

| 配置 | Projection | KKT | 预期结果 |
|------|-----------|-----|---------|
| Baseline | hard | false | 基线 |
| Learned-Prox (无 KKT) | learned | false | ⚠️ 性能下降 |
| Learned-Prox + KKT | learned | true | ✅ 性能提升 |

**结论**: Learned-Prox 必须与 KKT 组合使用

---

## 9. 具体修复建议

### 9.1 PDHG-Unroll 修复 (优先级: 🔴 高)

**文件**: `neupan/blocks/pdhg_unroll.py`

**修改 1: 添加自适应步长**
```python
class PDHGUnroll(nn.Module):
    def __init__(self, J=1, tau=0.5, sigma=0.5, learnable=False, adaptive=True):
        super().__init__()
        self.J = J
        self.adaptive = adaptive
        
        if learnable:
            self.tau = nn.Parameter(torch.tensor(tau))
            self.sigma = nn.Parameter(torch.tensor(sigma))
        else:
            self.register_buffer('tau', torch.tensor(tau))
            self.register_buffer('sigma', torch.tensor(sigma))
    
    def forward(self, mu, a, G):
        # 自适应步长
        if self.adaptive:
            G_norm = torch.linalg.matrix_norm(G, ord=2)
            tau_adaptive = self.tau / G_norm
            sigma_adaptive = self.sigma / G_norm
        else:
            tau_adaptive = self.tau
            sigma_adaptive = self.sigma
        
        # PDHG 迭代 (使用自适应步长)
        ...
```

**修改 2: 使用 SmoothClamp**
```python
def smooth_clamp(x, min_val, beta=10.0):
    """平滑的 clamp,避免梯度消失"""
    return torch.nn.functional.softplus((x - min_val) * beta) / beta + min_val

# 替换硬 clamp
y_norm_clamped = smooth_clamp(y_norm, 1.0)
mu = smooth_clamp(mu, 0.0)
v_norm = smooth_clamp(v_norm, 1.0)
```

**修改 3: 更新训练配置**
```yaml
# example/dune_train/dune_train_acker_pdhg-1_fixed.yaml
train:
  model_name: acker_learned_prox_pdhg-1_fixed_robot
  projection: learned
  use_kkt: true  # ✅ 启用 KKT
  w_kkt: 1e-3
  kkt_rho: 0.50
  unroll_J: 1
  pdhg_adaptive: true  # ✅ 启用自适应步长
  pdhg_tau: 0.5
  pdhg_sigma: 0.5
  batch_size: 256
  lr: 2.5e-6  # ✅ 降低学习率
  epoch: 5000
```

### 9.2 SE(2) 编码修复 (优先级: 🟡 中)

**文件**: `neupan/blocks/obs_point_net.py`

**修改: 添加归一化**
```python
def polar_embed(self, x: torch.Tensor, robot_scale: float = 1.0) -> torch.Tensor:
    r = torch.norm(x, dim=1, keepdim=True) / robot_scale  # ✅ 归一化
    theta = torch.atan2(x[:, 1], x[:, 0]).unsqueeze(1)
    return torch.cat([r, torch.cos(theta), torch.sin(theta)], dim=1)
```

**配置**: 针对 Acker 禁用 SE(2)
```yaml
# Acker 配置
se2_embed: false

# Diff 配置
se2_embed: true
robot_scale: 2.0  # Diff 的特征尺度
```

### 9.3 Learned-Prox 修复 (优先级: 🟢 低)

**配置**: 始终启用 KKT
```yaml
projection: learned
use_kkt: true  # ✅ 必需
w_kkt: 1e-3  # Acker
kkt_rho: 0.50
```

---

## 10. 总结与建议

### 10.1 核心发现

1. **PDHG-Unroll 是双刃剑**:
   - 推理时极其有效 (97.7% 改进)
   - 训练时需要自适应步长和 KKT 正则化

2. **Hard Projection 的作用被低估**:
   - 推理时看似无用
   - 训练时是 Learned-Prox 稳定性的基础

3. **机器人类型敏感性极高**:
   - Acker 需要 10 倍小的学习率
   - Acker 需要 10 倍大的 KKT 权重
   - SE(2) 不适用于 Acker

### 10.2 优先级建议

| 优先级 | 任务 | 预期收益 |
|--------|------|---------|
| 🔴 P0 | 修复 PDHG-Unroll (自适应步长) | 解锁 PDHG 消融实验 |
| 🔴 P0 | 为 Acker 调优超参数 | 提升 Acker 基线性能 |
| 🟡 P1 | 修复 SE(2) 编码 (归一化) | 解锁 SE(2) 消融实验 |
| 🟢 P2 | 文档化 Learned-Prox + KKT 必需性 | 避免未来错误配置 |

### 10.3 下一步行动

1. **立即执行**: 实现 PDHG 自适应步长
2. **验证**: 重新训练 Acker + PDHG-1 模型
3. **消融实验**: 使用修复后的模块进行完整消融实验
4. **文档**: 更新训练指南,明确各模块的使用条件

---

**报告结束**

