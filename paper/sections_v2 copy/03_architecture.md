# 3. PDPL-Net网络架构

本章详细阐述PDPL-Net的架构设计。PDPL-Net的设计理念并非简单的神经网络层级堆叠，而是对凸优化理论中Karush-Kuhn-Tucker（KKT）最优性条件的深度参数化建模。我们提出了一种"三位一体"的架构设计：**特征编码器**提供高质量的初始化，**PDHG展开层**通过交替更新逼近最优性，**硬投影层**则严格保障对偶可行性。这三个模块协同工作，使网络在极少的层数下便能输出既接近最优又严格可行的对偶变量。

## 3.1 总体架构

PDPL-Net的输入为障碍物点云$\mathcal{P} = \{\mathbf{p}_i\}_{i=1}^N$，其中每个点$\mathbf{p}_i \in \mathbb{R}^2$表示在机器人局部坐标系下的障碍物位置。网络的输出为对应的对偶变量$\{\mu_i\}_{i=1}^N$，其中$\mu_i \in \mathbb{R}^E$（$E$为机器人多边形的边数）。辅助变量$\lambda_i = -\mathbf{G}^\top \mu_i$可由$\mu_i$直接计算得到，因此网络只需预测$\mu$。

整体架构由三个级联模块组成。首先，**特征编码器**将每个点的二维坐标映射到高维特征空间，并回归出对偶变量的初始估计$\mu^{(0)}$和辅助变量$y^{(0)}$。这一初始化利用了问题的几何先验，为后续迭代提供了良好的起点。其次，**PDHG展开层**（共$J$层）通过交替更新原始变量和对偶变量来逼近KKT鞍点。每层展开对应原始-对偶混合梯度算法的一次迭代，但引入了可学习的残差模块以加速收敛。最后，**硬投影层**通过显式的几何投影操作——非负锥投影和二阶锥投影——将网络输出强制约束在可行域内，保证$\mu \geq 0$和$\|\mathbf{G}^\top \mu\|_2 \leq 1$。

从计算图的角度看，PDPL-Net是一个完全可微的端到端网络。在前向传播中，点云坐标依次经过编码、展开和投影得到可行的对偶变量；在反向传播中，梯度从损失函数出发依次穿过投影层、展开层和编码器，更新所有可学习参数。这种设计使得网络能够学习如何生成"自然可行"的解——即在投影前就已接近可行域，从而最大化利用网络容量学习最优性而非仅仅满足约束。

## 3.2 特征编码器

### 3.2.1 网络结构

传统优化算法通常从零向量或随机向量开始迭代，这忽略了不同问题实例之间的结构相似性。为了实现"热启动"（Warm Start），我们设计了一个几何感知的特征编码器，从输入点云中提取有意义的初始估计。编码器采用两层全连接网络结构：

$$
\mathbf{h} = \text{ReLU}(\mathbf{W}_2 \cdot \text{ReLU}(\mathbf{W}_1 \cdot \mathbf{p} + \mathbf{b}_1) + \mathbf{b}_2)
$$

其中，$\mathbf{p} \in \mathbb{R}^2$为输入点坐标，$\mathbf{h} \in \mathbb{R}^{32}$为隐藏特征。隐藏层维度设为32，在表达能力和计算效率之间取得了良好平衡。编码器的输出通过两个独立的线性头分别初始化$\mu^{(0)}$和$y^{(0)}$：

$$
\mu^{(0)} = \text{ReLU}(\mathbf{W}_\mu \mathbf{h} + \mathbf{b}_\mu), \quad y^{(0)} = \text{Tanh}(\mathbf{W}_y \mathbf{h} + \mathbf{b}_y)
$$

$\mu$头使用ReLU激活以保证初始值非负，$y$头使用Tanh激活以保证初始值有界。这种设计使得初始估计在物理意义上是合理的，减少了后续展开层需要修正的幅度。

### 3.2.2 SE(2)嵌入（可选）

为了增强网络对旋转变换的鲁棒性，我们提供了可选的SE(2)嵌入模块。该模块将笛卡尔坐标$(x, y)$转换为极坐标表示$(r, \cos\theta, \sin\theta)$：

$$
r = \sqrt{x^2 + y^2}, \quad \theta = \text{atan2}(y, x)
$$

极坐标表示的优点在于其对坐标系旋转的不变性：当机器人旋转时，障碍物点的角度$\theta$会相应变化，但$(r, \cos\theta, \sin\theta)$的组合能够被网络更容易地学习对应关系。实验表明，SE(2)嵌入在某些场景下能够提升泛化性能，但对于大多数情况，直接使用笛卡尔坐标已足够有效。

## 3.3 PDHG展开层

### 3.3.1 PDHG算法回顾

原始-对偶混合梯度（Primal-Dual Hybrid Gradient, PDHG）算法是求解鞍点问题的一阶方法，由Chambolle和Pock于2011年提出。对于我们的对偶优化问题，PDHG迭代可以写成以下形式：

$$
\begin{cases}
y^{(k+1)} = \mathcal{P}_{\mathcal{B}_2}(y^{(k)} + \sigma \mathbf{G}^\top \mu^{(k)}) \\
\mu^{(k+1)} = \mathcal{P}_{\mathbb{R}_+^E}(\mu^{(k)} + \tau (\mathbf{a} - \mathbf{G} y^{(k+1)}))
\end{cases}
$$

其中，$\mathbf{a} = \mathbf{p}^\top \mathbf{G}^\top - \mathbf{g}^\top$为问题的线性系数，$\tau$和$\sigma$为步长参数，$\mathcal{P}_{\mathcal{B}_2}$和$\mathcal{P}_{\mathbb{R}_+^E}$分别为到单位球和非负锥的投影算子。PDHG算法的优点在于其简洁的更新规则和稳定的收敛性质——在步长参数满足$\tau\sigma\|\mathbf{G}\|^2 < 1$时保证收敛。

### 3.3.2 展开策略

PDPL-Net将PDHG迭代展开为$J$层神经网络，每层对应一次完整的原始-对偶更新。与标准PDHG不同，我们引入了可学习的残差模块$\mathcal{R}_\theta$来加速收敛：

$$
\begin{cases}
y^{(j+1)} = \mathcal{P}_{\mathcal{B}_2}(y^{(j)} + \sigma_j \mathbf{G}^\top \mu^{(j)}) \\
\tilde{\mu}^{(j+1)} = \mu^{(j)} + \tau_j (\mathbf{a} - \mathbf{G} y^{(j+1)}) \\
\mu^{(j+1)} = \mathcal{P}_{dual}(\tilde{\mu}^{(j+1)} + \alpha \cdot \mathcal{R}_\theta(\mathbf{G}^\top \tilde{\mu}^{(j+1)}))
\end{cases}
$$

其中，$\tau_j$和$\sigma_j$为每层独立的步长参数（固定或可学习），$\alpha \in (0, 1)$为残差缩放因子（默认0.5），$\mathcal{P}_{dual}$为到对偶可行域的投影（详见3.4节）。残差模块$\mathcal{R}_\theta$是一个轻量级MLP，输入为$\mathbf{G}^\top \mu$（2维），输出为$\mu$的修正量（$E$维）。

这种展开设计的关键洞见在于：标准PDHG需要数十至数百次迭代才能收敛，是因为步长参数必须设得非常保守以保证最坏情况下的收敛。而残差模块通过数据驱动的方式学习了问题结构的先验知识，能够预测出比标准梯度更优的更新方向。从优化理论的角度看，这相当于网络学习了一个自适应的预处理器，将一阶梯度下降转变为类似牛顿法的二阶优化过程，从而在极少的层数（$J=1\sim3$）下实现收敛。

### 3.3.3 可学习近端算子

残差模块$\mathcal{R}_\theta$可以理解为一个可学习的近端算子（Learned Proximal Operator）。在传统近端算法中，近端算子定义为：

$$
\text{prox}_f(x) = \arg\min_z \left\{ f(z) + \frac{1}{2}\|z - x\|^2 \right\}
$$

对于复杂的目标函数$f$，精确计算近端算子可能非常困难。我们的方法用神经网络$\mathcal{R}_\theta$来近似这个算子，使其能够根据输入特征自适应地输出最优的修正方向。残差模块的结构设计为两层全连接网络：

$$
\mathcal{R}_\theta(z) = \mathbf{W}_2 \cdot \text{ReLU}(\mathbf{W}_1 z + \mathbf{b}_1) + \mathbf{b}_2
$$

输入$z = \mathbf{G}^\top \mu \in \mathbb{R}^2$，隐藏层维度为32，输出维度为$E$。整个残差模块的参数量约为200，仅占网络总参数的10%左右，但对收敛速度有显著的加速效果。消融实验表明，移除可学习近端算子后，网络需要更多的展开层才能达到相同精度。

## 3.4 硬投影层

### 3.4.1 设计动机

硬投影层是PDPL-Net区别于其他深度展开方法的核心设计。现有的展开网络（如LISTA、ADMM-Net）大多通过在损失函数中添加约束违背的罚项来处理约束，即所谓的"软约束"方法。这种方法存在根本性缺陷：罚系数需要精心调参，且无论罚系数设置多大，都无法保证测试时的严格约束满足。对于安全攸关的机器人控制任务，这种概率性的约束满足是不可接受的。

我们的实验表明，如果仅使用监督学习损失训练PointNet++或MLP来预测对偶变量，约束违背率高达20–30%。即使在损失函数中添加约束惩罚项，违背率仍然在5–10%左右。这证实了在高维空间中，约束可行域的边界是一个测度为零的集合，仅靠神经网络的拟合能力无法精确"命中"这一边界。因此，我们需要在网络架构层面嵌入显式的投影操作，从根本上消除约束违背的可能性。

### 3.4.2 投影操作

硬投影层通过两步串联投影将网络输出强制约束在对偶可行域$\mathcal{C}_{dual} = \{\mu \geq 0, \|\mathbf{G}^\top \mu\|_2 \leq 1\}$内：

**步骤1：非负锥投影**。首先将$\mu$的每个分量投影到非负实数：
$$
\hat{\mu} = \max(0, \mu) = \text{ReLU}(\mu)
$$
这是一个逐元素的操作，计算复杂度为$O(E)$。在深度学习框架中，ReLU激活函数已被高度优化，几乎不增加计算开销。

**步骤2：二阶锥缩放**。然后对$\hat{\mu}$进行范数归一化，确保$\|\mathbf{G}^\top \mu\|_2 \leq 1$：
$$
\mu^* = \frac{\hat{\mu}}{\max(1, \|\mathbf{G}^\top \hat{\mu}\|_2)}
$$
这一步首先计算$v = \mathbf{G}^\top \hat{\mu}$的范数，如果范数超过1则按比例缩放整个向量，否则保持不变。$\max(1, \cdot)$操作确保了当$\mu$已经可行时不做任何修改（幂等性）。

**可行性保证**。经过上述两步投影后，输出$\mu^*$满足：（1）$\mu^* \geq 0$，因为ReLU保证了非负性，而后续的缩放操作不改变符号；（2）$\|\mathbf{G}^\top \mu^*\|_2 \leq 1$，因为范数归一化显式地将向量拉回单位球内。因此，无论网络中间层输出如何发散，最终的$\mu^*$恒定属于对偶可行域。

### 3.4.3 梯度传播

一个潜在的担忧是硬投影操作（特别是ReLU和$\max$函数）会破坏梯度的传播，影响端到端训练。实际上，这种担忧是不必要的。ReLU函数在$x > 0$时梯度为1，在$x < 0$时梯度为0，是分段线性的；范数归一化在球内时梯度为恒等映射，在球外时梯度是关于归一化因子的除法链式法则。两个操作在其作用区域内都是光滑的，仅在边界处（$x = 0$或$\|v\| = 1$）存在梯度不连续。由于边界是零测集合，训练过程中绝大多数样本的梯度能够有效传播。

实验中我们观察到一个有趣的现象：随着训练的进行，网络输出被投影截断的比例逐渐减小。在训练初期，约30%的样本需要非负投影修正，约20%需要范数归一化修正；而在训练收敛后，这些比例分别降低到5%和2%以下。这表明网络学会了主动生成可行的输出，投影层从"主动修正"的角色转变为"安全兜底"的角色。

## 3.5 网络参数汇总

表1汇总了PDPL-Net的网络结构和参数量。整个网络极其轻量级，总参数量约为1600（$J=2$时），远小于PointNet++（数百万参数）和Point Transformer（数百万至数千万参数）。这种轻量级设计不仅带来了推理速度的优势，还降低了过拟合的风险。

**表1：PDPL-Net网络架构参数详情**

| 模块 | 组件 | 配置 | 参数量 |
|:-----|:-----|:-----|-------:|
| **Feature Encoder** | MLP Layer 1 | 输入: 2, 隐藏: 32, ReLU | ~100 |
| | MLP Layer 2 | 隐藏: 32, 输出: 32, ReLU | ~1000 |
| | Init Head ($\mu$) | 输入: 32, 输出: $E$ | ~150 |
| | Init Head ($y$) | 输入: 32, 输出: 2 | ~70 |
| **PDHG Unrolling** | Residual Module | 输入: 2, 隐藏: 32, 输出: $E$ | ~200×$J$ |
| ($J$ layers) | Step Sizes | $\tau, \sigma$ per layer | 2×$J$ |
| **Hard Projection** | ReLU + Normalization | 无参数 | 0 |
| **Total** | | $J=2$, $E=4$ | **~1,600** |

